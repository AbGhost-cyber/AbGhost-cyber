{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNKKy65t9l/E7EwWfefiKrl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbGhost-cyber/AbGhost-cyber/blob/main/siamTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "4nhh_-I531ID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def accuracy(distances, y, step=0.01):\n",
        "    min_threshold_d = min(distances)\n",
        "    max_threshold_d = max(distances)\n",
        "    max_acc = 0\n",
        "    same_id = (y == 1)\n",
        "\n",
        "    for threshold_d in torch.arange(min_threshold_d, max_threshold_d + step, step):\n",
        "        true_positive = (distances <= threshold_d) & (same_id)\n",
        "        true_positive_rate = true_positive.sum().float() / same_id.sum().float()\n",
        "        true_negative = (distances > threshold_d) & (~same_id)\n",
        "        true_negative_rate = true_negative.sum().float() / (~same_id).sum().float()\n",
        "\n",
        "        acc = 0.5 * (true_negative_rate + true_positive_rate)\n",
        "        max_acc = max(max_acc, acc)\n",
        "    return max_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "seed = 2020\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "def get_data_loader(is_train, batch_size, image_transform, dataset='cedar'):\n",
        "    if dataset == 'cedar':\n",
        "        data_dir = './data/CEDAR'\n",
        "    elif dataset == 'bengali':\n",
        "        data_dir = './data/BHSig260/Bengali'\n",
        "    elif dataset == 'hindi':\n",
        "        data_dir = './data/BHSig260/Hindi'\n",
        "    else:\n",
        "        raise ValueError(f'Unknow dataset {dataset}')\n",
        "    data = SignDataset(is_train, data_dir, image_transform)\n",
        "    is_shuffle = is_train\n",
        "    loader = DataLoader(data, batch_size=batch_size, shuffle=is_shuffle, num_workers=4, pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "class SignDataset(Dataset):\n",
        "    def __init__(self, is_train: bool, data_dir: str, image_transform=None):\n",
        "        if not os.path.exists(os.path.join(data_dir, 'train.csv')) or not os.path.exists(\n",
        "                os.path.join(data_dir, 'test.csv')):\n",
        "            print('Not found train/test splits, run create_annotation first')\n",
        "        else:\n",
        "            print('Use existed train/test splits')\n",
        "\n",
        "        if is_train:\n",
        "            self.df = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None)\n",
        "        else:\n",
        "            self.df = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None)\n",
        "\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x1, x2, y = self.df.iloc[index]\n",
        "\n",
        "        x1 = Image.open(x1).convert('L')\n",
        "        x2 = Image.open(x2).convert('L')\n",
        "\n",
        "        if self.image_transform:\n",
        "            x1 = self.image_transform(x1)\n",
        "            x2 = self.image_transform(x2)\n",
        "\n",
        "        return x1, x2, y"
      ],
      "metadata": {
        "id": "6mnDGsU25CDH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, alpha, beta, margin):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, x1, x2, y):\n",
        "        '''\n",
        "        Shapes:\n",
        "        -------\n",
        "        x1: [B,C]\n",
        "        x2: [B,C]\n",
        "        y: [B,1]\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        loss: [B,1]]\n",
        "        '''\n",
        "        distance = torch.pairwise_distance(x1, x2, p=2)\n",
        "        loss = self.alpha * (1 - y) * distance ** 2 + \\\n",
        "               self.beta * y * (torch.max(torch.zeros_like(distance), self.margin - distance) ** 2)\n",
        "        return torch.mean(loss, dtype=torch.float)\n",
        "\n",
        "\n",
        "class SigNet(nn.Module):\n",
        "    '''\n",
        "    Reference Keras: https://github.com/sounakdey/SigNet/blob/master/SigNet_v1.py\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # self.features = nn.Sequential(\n",
        "        #     # input size = [155, 220, 1]\n",
        "        #     nn.Conv2d(1, 96, 11),  # size = [145,210]\n",
        "        #     nn.ReLU(),\n",
        "        #     nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "        #     nn.MaxPool2d(2, stride=2),  # size = [72, 105]\n",
        "        #     nn.Conv2d(96, 256, 5, padding=2, padding_mode='zeros'),  # size = [72, 105]\n",
        "        #     nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "        #     nn.MaxPool2d(2, stride=2),  # size = [36, 52]\n",
        "        #     nn.Dropout2d(p=0.3),\n",
        "        #     nn.Conv2d(256, 384, 3, stride=1, padding=1, padding_mode='zeros'),\n",
        "        #     nn.Conv2d(384, 256, 3, stride=1, padding=1, padding_mode='zeros'),\n",
        "        #     nn.MaxPool2d(2, stride=2),  # size = [18, 26]\n",
        "        #     nn.Dropout2d(p=0.3),\n",
        "        #     nn.Flatten(1, -1),  # 18*26*256\n",
        "        #     nn.LazyLinear(18 * 26 * 256, 1024),\n",
        "        #     nn.LazyLinear(1024),\n",
        "        #     nn.Dropout2d(p=0.5),\n",
        "        #     nn.Linear(1024, 128),\n",
        "        # )\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.LazyConv2d(96, kernel_size=7, padding=2, padding_mode='zeros', bias=False), nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            nn.LazyConv2d(128, kernel_size=5, padding=2, padding_mode='zeros', bias=False), nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            nn.LazyConv2d(224, kernel_size=3, padding=2, padding_mode='zeros', bias=False), nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.AvgPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(p=0.3),\n",
        "\n",
        "            nn.LazyConv2d(384, kernel_size=3, padding=2, padding_mode='zeros', bias=False), nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.LazyLinear(1024), nn.ReLU(),\n",
        "            nn.Dropout(p=0.5),\n",
        "            nn.LazyLinear(128))\n",
        "\n",
        "        # TODO: init bias = 0\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.features(x1)\n",
        "        x2 = self.features(x2)\n",
        "        return x1, x2\n"
      ],
      "metadata": {
        "id": "F0qbazxf5D09"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import ImageOps\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "seed = 2020\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "\n",
        "def train(model, optimizer, criterion, dataloader, log_interval=50):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    number_samples = 0\n",
        "\n",
        "    for batch_idx, (x1, x2, y) in enumerate(dataloader):\n",
        "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        x1, x2 = model(x1, x2)\n",
        "        loss = criterion(x1, x2, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        number_samples += len(x1)\n",
        "        running_loss += loss.item() * len(x1)\n",
        "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(dataloader) - 1:\n",
        "            print('{}/{}: Loss: {:.4f}'.format(batch_idx + 1, len(dataloader), running_loss / number_samples))\n",
        "            running_loss = 0\n",
        "            number_samples = 0\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dw45rlos5FgZ",
        "outputId": "82707f3e-192a-408f-9110-c34483b50a69"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval(model, criterion, dataloader, log_interval=50):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    number_samples = 0\n",
        "\n",
        "    distances = []\n",
        "\n",
        "    for batch_idx, (x1, x2, y) in enumerate(dataloader):\n",
        "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
        "\n",
        "        x1, x2 = model(x1, x2)\n",
        "        loss = criterion(x1, x2, y)\n",
        "        distances.extend(zip(torch.pairwise_distance(x1, x2, 2).cpu().tolist(), y.cpu().tolist()))\n",
        "\n",
        "        number_samples += len(x1)\n",
        "        running_loss += loss.item() * len(x1)\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(dataloader) - 1:\n",
        "            print('{}/{}: Loss: {:.4f}'.format(batch_idx + 1, len(dataloader), running_loss / number_samples))\n",
        "\n",
        "    distances, y = zip(*distances)\n",
        "    distances, y = torch.tensor(distances), torch.tensor(y)\n",
        "    max_accuracy = accuracy(distances, y)\n",
        "    print(f'Max accuracy: {max_accuracy}')\n",
        "    return running_loss / number_samples, max_accuracy"
      ],
      "metadata": {
        "id": "7S9UmWhf5HR7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "lr = 1e-5\n",
        "dataset = 'cedar'"
      ],
      "metadata": {
        "id": "06aseoR_5JLj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SigNet().to(device)\n",
        "criterion = ContrastiveLoss(alpha=1, beta=1, margin=1).to(device)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=5e-4, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 5, 0.1)\n",
        "num_epochs = 20\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "        transforms.Resize((50, 50)),\n",
        "        ImageOps.invert,\n",
        "        transforms.ToTensor(),\n",
        "        # TODO: add normalize\n",
        "    ])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2z7P7jg25KyW",
        "outputId": "4740faeb-986f-4829-a5f1-be4be5413360"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
            "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = get_data_loader(is_train=True, batch_size=batch_size, image_transform=image_transform,\n",
        "                                  dataset=dataset)\n",
        "testloader = get_data_loader(is_train=False, batch_size=batch_size, image_transform=image_transform,\n",
        "                                 dataset=dataset)\n",
        "os.makedirs('checkpoints', exist_ok=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gpqh86kb5M0w",
        "outputId": "791b6d5b-08a2-44b1-c0e2-50c9eafe208b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use existed train/test splits\n",
            "Use existed train/test splits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "model.train()\n",
        "print(model)\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('Training', '-' * 20)\n",
        "    train(model, optimizer, criterion, trainloader)\n",
        "    print('Evaluating', '-' * 20)\n",
        "    loss, acc = eval(model, criterion, testloader)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    scheduler.step()\n",
        "\n",
        "#     to_save = {\n",
        "#         'model': model.state_dict(),\n",
        "#         'scheduler': scheduler.state_dict(),\n",
        "#         'optim': optimizer.state_dict(),\n",
        "#     }\n",
        "\n",
        "#     print('Saving checkpoint..')\n",
        "#     torch.save(to_save, 'checkpoints/epoch_{}_loss_{:.3f}_acc_{:.3f}.pt'.format(epoch, loss, acc))\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 971
        },
        "id": "8ylyMIls5PTe",
        "outputId": "acc41ae2-b468-4a71-bb7a-2b7dce5fbb1e"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SigNet(\n",
            "  (features): Sequential(\n",
            "    (0): LazyConv2d(0, 96, kernel_size=(7, 7), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (1): ReLU()\n",
            "    (2): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "    (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (4): Dropout(p=0.3, inplace=False)\n",
            "    (5): LazyConv2d(0, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (6): ReLU()\n",
            "    (7): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "    (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (9): Dropout(p=0.3, inplace=False)\n",
            "    (10): LazyConv2d(0, 224, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (11): ReLU()\n",
            "    (12): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "    (13): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
            "    (14): Dropout(p=0.3, inplace=False)\n",
            "    (15): LazyConv2d(0, 384, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), bias=False)\n",
            "    (16): ReLU()\n",
            "    (17): LocalResponseNorm(5, alpha=0.0001, beta=0.75, k=2)\n",
            "    (18): Flatten(start_dim=1, end_dim=-1)\n",
            "    (19): LazyLinear(in_features=0, out_features=1024, bias=True)\n",
            "    (20): ReLU()\n",
            "    (21): Dropout(p=0.5, inplace=False)\n",
            "    (22): LazyLinear(in_features=0, out_features=128, bias=True)\n",
            "  )\n",
            ")\n",
            "Epoch 0/20\n",
            "Training --------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-1f4a0b5ff40f>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {}/{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Evaluating'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-df8a5ea00c9e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, dataloader, log_interval)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    201\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 122.00 MiB (GPU 0; 14.75 GiB total capacity; 13.47 GiB already allocated; 40.81 MiB free; 13.65 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the accuracy\n",
        "plt.plot(accuracies)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ABinJvl5VEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}