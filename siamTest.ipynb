{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP9nYvmHU0L6uDjPAOWKFbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AbGhost-cyber/AbGhost-cyber/blob/main/siamTest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nhh_-I531ID"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "def accuracy(distances, y, step=0.01):\n",
        "    min_threshold_d = min(distances)\n",
        "    max_threshold_d = max(distances)\n",
        "    max_acc = 0\n",
        "    same_id = (y == 1)\n",
        "\n",
        "    for threshold_d in torch.arange(min_threshold_d, max_threshold_d + step, step):\n",
        "        true_positive = (distances <= threshold_d) & (same_id)\n",
        "        true_positive_rate = true_positive.sum().float() / same_id.sum().float()\n",
        "        true_negative = (distances > threshold_d) & (~same_id)\n",
        "        true_negative_rate = true_negative.sum().float() / (~same_id).sum().float()\n",
        "\n",
        "        acc = 0.5 * (true_negative_rate + true_positive_rate)\n",
        "        max_acc = max(max_acc, acc)\n",
        "    return max_acc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip full_forg.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WbphgD-6ZQS",
        "outputId": "25e9d4ad-d41b-4100-a53b-5a8cee68c7c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  full_forg.zip\n",
            "  End-of-central-directory signature not found.  Either this file is not\n",
            "  a zipfile, or it constitutes one disk of a multi-part archive.  In the\n",
            "  latter case the central directory and zipfile comment will be found on\n",
            "  the last disk(s) of this archive.\n",
            "unzip:  cannot find zipfile directory in one of full_forg.zip or\n",
            "        full_forg.zip.zip, and cannot find full_forg.zip.ZIP, period.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "seed = 2020\n",
        "np.random.seed(seed)\n",
        "\n",
        "\n",
        "def get_data_loader(is_train, batch_size, image_transform, dataset='cedar'):\n",
        "    if dataset == 'cedar':\n",
        "        data_dir = './data/CEDAR'\n",
        "    elif dataset == 'bengali':\n",
        "        data_dir = './data/BHSig260/Bengali'\n",
        "    elif dataset == 'hindi':\n",
        "        data_dir = './data/BHSig260/Hindi'\n",
        "    else:\n",
        "        raise ValueError(f'Unknow dataset {dataset}')\n",
        "    data = SignDataset(is_train, data_dir, image_transform)\n",
        "    is_shuffle = is_train\n",
        "    loader = DataLoader(data, batch_size=batch_size, shuffle=is_shuffle, num_workers=4, pin_memory=True)\n",
        "    return loader\n",
        "\n",
        "\n",
        "class SignDataset(Dataset):\n",
        "    def __init__(self, is_train: bool, data_dir: str, image_transform=None):\n",
        "        if not os.path.exists(os.path.join(data_dir, 'train.csv')) or not os.path.exists(\n",
        "                os.path.join(data_dir, 'test.csv')):\n",
        "            print('Not found train/test splits, run create_annotation first')\n",
        "        else:\n",
        "            print('Use existed train/test splits')\n",
        "\n",
        "        if is_train:\n",
        "            self.df = pd.read_csv(os.path.join(data_dir, 'train.csv'), header=None)\n",
        "        else:\n",
        "            self.df = pd.read_csv(os.path.join(data_dir, 'test.csv'), header=None)\n",
        "\n",
        "        self.image_transform = image_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x1, x2, y = self.df.iloc[index]\n",
        "\n",
        "        x1 = Image.open(x1).convert('L')\n",
        "        x2 = Image.open(x2).convert('L')\n",
        "\n",
        "        if self.image_transform:\n",
        "            x1 = self.image_transform(x1)\n",
        "            x2 = self.image_transform(x2)\n",
        "\n",
        "        return x1, x2, y"
      ],
      "metadata": {
        "id": "6mnDGsU25CDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchvision import models\n",
        "\n",
        "\n",
        "class ContrastiveLoss(nn.Module):\n",
        "    def __init__(self, alpha, beta, margin):\n",
        "        super().__init__()\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.margin = margin\n",
        "\n",
        "    def forward(self, x1, x2, y):\n",
        "        '''\n",
        "        Shapes:\n",
        "        -------\n",
        "        x1: [B,C]\n",
        "        x2: [B,C]\n",
        "        y: [B,1]\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        loss: [B,1]]\n",
        "        '''\n",
        "        distance = torch.pairwise_distance(x1, x2, p=2)\n",
        "        loss = self.alpha * (1 - y) * distance ** 2 + \\\n",
        "               self.beta * y * (torch.max(torch.zeros_like(distance), self.margin - distance) ** 2)\n",
        "        return torch.mean(loss, dtype=torch.float)\n",
        "\n",
        "\n",
        "class SigNet(nn.Module):\n",
        "    '''\n",
        "    Reference Keras: https://github.com/sounakdey/SigNet/blob/master/SigNet_v1.py\n",
        "    '''\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(\n",
        "            # input size = [155, 220, 1]\n",
        "            nn.Conv2d(1, 96, 11),  # size = [145,210]\n",
        "            nn.ReLU(),\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.MaxPool2d(2, stride=2),  # size = [72, 105]\n",
        "            nn.Conv2d(96, 256, 5, padding=2, padding_mode='zeros'),  # size = [72, 105]\n",
        "            nn.LocalResponseNorm(size=5, k=2, alpha=1e-4, beta=0.75),\n",
        "            nn.MaxPool2d(2, stride=2),  # size = [36, 52]\n",
        "            # nn.Dropout2d(p=0.3),\n",
        "            nn.Conv2d(256, 384, 3, stride=1, padding=1, padding_mode='zeros'),\n",
        "            nn.Conv2d(384, 256, 3, stride=1, padding=1, padding_mode='zeros'),\n",
        "            nn.MaxPool2d(2, stride=2),  # size = [18, 26]\n",
        "            # nn.Dropout2d(p=0.3),\n",
        "            nn.Flatten(1, -1),  # 18*26*256\n",
        "            # nn.LazyLinear(18 * 26 * 256, 1024),\n",
        "            nn.LazyLinear(1024),\n",
        "            # nn.Dropout2d(p=0.5),\n",
        "            nn.Linear(1024, 128),\n",
        "        )\n",
        "\n",
        "        # TODO: init bias = 0\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.features(x1)\n",
        "        x2 = self.features(x2)\n",
        "        return x1, x2\n"
      ],
      "metadata": {
        "id": "F0qbazxf5D09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import ImageOps\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "seed = 2020\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "\n",
        "def train(model, optimizer, criterion, dataloader, log_interval=50):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    number_samples = 0\n",
        "\n",
        "    for batch_idx, (x1, x2, y) in enumerate(dataloader):\n",
        "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        x1, x2 = model(x1, x2)\n",
        "        loss = criterion(x1, x2, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        number_samples += len(x1)\n",
        "        running_loss += loss.item() * len(x1)\n",
        "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(dataloader) - 1:\n",
        "            print('{}/{}: Loss: {:.4f}'.format(batch_idx + 1, len(dataloader), running_loss / number_samples))\n",
        "            running_loss = 0\n",
        "            number_samples = 0\n"
      ],
      "metadata": {
        "id": "Dw45rlos5FgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def eval(model, criterion, dataloader, log_interval=50):\n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    number_samples = 0\n",
        "\n",
        "    distances = []\n",
        "\n",
        "    for batch_idx, (x1, x2, y) in enumerate(dataloader):\n",
        "        x1, x2, y = x1.to(device), x2.to(device), y.to(device)\n",
        "\n",
        "        x1, x2 = model(x1, x2)\n",
        "        loss = criterion(x1, x2, y)\n",
        "        distances.extend(zip(torch.pairwise_distance(x1, x2, 2).cpu().tolist(), y.cpu().tolist()))\n",
        "\n",
        "        number_samples += len(x1)\n",
        "        running_loss += loss.item() * len(x1)\n",
        "\n",
        "        if (batch_idx + 1) % log_interval == 0 or batch_idx == len(dataloader) - 1:\n",
        "            print('{}/{}: Loss: {:.4f}'.format(batch_idx + 1, len(dataloader), running_loss / number_samples))\n",
        "\n",
        "    distances, y = zip(*distances)\n",
        "    distances, y = torch.tensor(distances), torch.tensor(y)\n",
        "    max_accuracy = accuracy(distances, y)\n",
        "    print(f'Max accuracy: {max_accuracy}')\n",
        "    return running_loss / number_samples, max_accuracy"
      ],
      "metadata": {
        "id": "7S9UmWhf5HR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "lr = 1e-5\n",
        "dataset = 'cedar'"
      ],
      "metadata": {
        "id": "06aseoR_5JLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SigNet().to(device)\n",
        "criterion = ContrastiveLoss(alpha=1, beta=1, margin=1).to(device)\n",
        "optimizer = optim.RMSprop(model.parameters(), lr=1e-5, eps=1e-8, weight_decay=5e-4, momentum=0.9)\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, 5, 0.1)\n",
        "num_epochs = 20\n",
        "\n",
        "image_transform = transforms.Compose([\n",
        "        transforms.Resize((100, 100)),\n",
        "        ImageOps.invert,\n",
        "        transforms.ToTensor(),\n",
        "        # TODO: add normalize\n",
        "    ])"
      ],
      "metadata": {
        "id": "2z7P7jg25KyW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainloader = get_data_loader(is_train=True, batch_size=batch_size, image_transform=image_transform,\n",
        "                                  dataset=dataset)\n",
        "testloader = get_data_loader(is_train=False, batch_size=batch_size, image_transform=image_transform,\n",
        "                                 dataset=dataset)\n",
        "os.makedirs('checkpoints', exist_ok=True)"
      ],
      "metadata": {
        "id": "Gpqh86kb5M0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "accuracies = []\n",
        "\n",
        "model.train()\n",
        "print(model)\n",
        "for epoch in range(num_epochs):\n",
        "    print('Epoch {}/{}'.format(epoch, num_epochs))\n",
        "    print('Training', '-' * 20)\n",
        "    train(model, optimizer, criterion, trainloader)\n",
        "    print('Evaluating', '-' * 20)\n",
        "    loss, acc = eval(model, criterion, testloader)\n",
        "    losses.append(loss)\n",
        "    accuracies.append(acc)\n",
        "    scheduler.step()\n",
        "\n",
        "#     to_save = {\n",
        "#         'model': model.state_dict(),\n",
        "#         'scheduler': scheduler.state_dict(),\n",
        "#         'optim': optimizer.state_dict(),\n",
        "#     }\n",
        "\n",
        "#     print('Saving checkpoint..')\n",
        "#     torch.save(to_save, 'checkpoints/epoch_{}_loss_{:.3f}_acc_{:.3f}.pt'.format(epoch, loss, acc))\n",
        "\n",
        "print('Done')"
      ],
      "metadata": {
        "id": "8ylyMIls5PTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "# Plotting the accuracy\n",
        "plt.plot(accuracies)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training Accuracy')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "8ABinJvl5VEO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}